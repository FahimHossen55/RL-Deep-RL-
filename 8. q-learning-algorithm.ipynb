{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Q learning \n- Q lerarning is a type of Reinforcement learning that is both off policy and model free. The goal of the algorithm is to determine the optimal action to take based on the current state of the agent within its environment \n\n- The algorithm may generate its own rules or operate outside of any pre existing policy,. Because it is off policy. There is no strict requirement for a policy to be in place.\n\n-  Furthermore, the agent utilizes a model free approches by predicting the expected response of the environment. it doesnt rely solely on rewards to learn but employs trail and error to make decisions.\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"Q table IS just a fancy name for a simple lookup table where we calculate the maximum expected future rewards for action at each state. Basically, this table will guide us to the best action at each state. There will be four numbers of actions at each non edge tile. <br>  \nif there is a risk of a large reward close to the optimal path, Q learning will tend to trigger that reward while exploring And in practice. if the mistakes are costly , you dont want  Q learning to explore more to these negative rewards. You will want something more conservative.  ","metadata":{}},{"cell_type":"markdown","source":"## Q LEARNING: WORKING\n\n### Here are the steps involved in ## Q-learning algorithm:\n\n\n\n- Initialize the Q-values for all state-action pairs to some arbitrary value.\n- Set the learning rate (alpha) and discount factor (gamma) to some appropriate values.\n- Observe the current state s.\n- Choose an action a based on the current state s using an exploration-exploitation\n- strategy like epsilon-greedy or softmax.\n- Take the chosen action a and observe the reward r and the next state s'.\n- Update the Q-value for the current state-action pair using the Bellman equation:\n- q(s,a)=q(s,a)+α[r+γmaxq(s',a')-q(s,a)]\n- Repeat steps 3-6 until convergence or for a fixed number of iterations.\n- Use the learned Q-values to select actions in the future by choosing the action that\n- maximizes the Q-value for the current state.\n\n<br> \nIn summary, Q-learning works by repeatedly updating the Q-values for state-action pairs based on the observed rewards and the expected future rewards. The algorithm gradually learns the optimal policy that maximizes the expected cumulative reward.\n\n\n\n\n\n","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}