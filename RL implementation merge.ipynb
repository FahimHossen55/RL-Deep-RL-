{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff40606e",
   "metadata": {},
   "source": [
    "## **Elements:** \n",
    "- **Agent :**  The entity to learns to make decesions in the Environment\n",
    "- **Environment:** The external world that the agent interects with\n",
    "- **State** : Position in envionment\n",
    "- **Action**: The decesion made by the agent in responce to the environment\n",
    "- **Reward** :  The feedback received by the agent from the environment based on its action\n",
    "- **Policy** : The Strategy used by the agent from the environment\n",
    "- **Value** :  Goodness of future reward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ff4915",
   "metadata": {},
   "source": [
    "##  Way to implement : \n",
    "- ### Model-free RL \n",
    "- ### Model based RL \n",
    "- ### Deep reinforcement learning \n",
    "- ###  Actor-Critic Methods \n",
    "- ### Meta RL ( Ex, MAML model Agnostic MMeta learning ) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb5c8cc",
   "metadata": {},
   "source": [
    "## STATE VECTOR "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9c8f2c",
   "metadata": {},
   "source": [
    "in RL , A state vector is a numerical representaton of the current state of the environment that an agent uses to make decesions about what action to take \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35747207",
   "metadata": {},
   "source": [
    "## Policy \n",
    "A policy is a mapping between states of the environment and the actions to be taken in those states. it specifies what action the agent should take in a given state to maximize its expected cumulative reward over time \n",
    "- A dereministic policy \n",
    "- A probabilistic policy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f578d1",
   "metadata": {},
   "source": [
    "## Exploration & Exploitation \n",
    "- ### Exploration :\n",
    "- Exploration  is the new techinique of trying out new actions to gain information about the environment . it allows the agent to discover new states and learn How to behave in those states However excessive exploration may prevent the agent from exploitating its current knowledge of the environment\n",
    "- ### Exploitation :\n",
    "-  Exploitation is the process of taking actions that agent believes will maximize its expected cumulative reward based on its current knowledge of the environment. it allows the agent to maximize its reward in short term.\n",
    "-   To find optimal policy. the agent must balane exploration and exploitation. it must explore enough to discover new states and actions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e74d1a",
   "metadata": {},
   "source": [
    "##  Exploration - Exploitation TRADE OFF \n",
    "\n",
    "- Epsilon greedy: This is a strategy that balance exploration and exploitation by choosing the best action with high probability and choosing a random action with low probability\n",
    "- Boltzman Exploitation: Boltzman exploitation is a strategy that balance exploration and exploitaion by assigning  probabilistics to each action based on its expected reward and choosing actions probabilistically based on these probabilistics\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e33fa11",
   "metadata": {},
   "source": [
    "## Markov STATE \n",
    "A markov state is a state in  a reinforcement learning problem that satiffy the markov assumption which states that the currenty state contains all the necessary information for determining the optimal next action with requiring any past informatation beyond the current state. \n",
    "\n",
    "According to the markov assumption, the current state of an agent in a agent in reinforcement leaning problem contains all the relevent infoermation about past state and actions  this means that the current stae is considered sufficient for determining the optimal next action , with needing to consider any past information beyond the current state. \n",
    "- in a word :  Forcus on current state to for the future \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b47daf1",
   "metadata": {},
   "source": [
    "#### Example : investment Portfolio \n",
    "- GDP\n",
    "- interest rate\n",
    "- Market trend\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3a8de2",
   "metadata": {},
   "source": [
    "## MDP :  Markov decesion Process\n",
    "\n",
    "MDPs provide a mathematical framework to descrive the envionment in reinforcement learning and they can be used to model almost all RL problems. \n",
    "#### Contains: \n",
    "- possible states\n",
    "- Possible actions\n",
    "- Reward functions\n",
    "- Transation\n",
    "\n",
    "- An MDP is made up of a set of finite environ states, a set of possible actions in each state, A real valued reward function and a transition model. Howwevar, real world environment often lack prior knowledge of their dynamics. \n",
    "\n",
    "## HMM ( Hidden markov Model ): \n",
    "A hidden markov model (HMM) is statistical model used to represent a stochastic process, where the underlying system is assumed to be a markov process with unknown parameter. HMMs are widely used in fields such as signal processibng , speech recognition , NLP , bioinformatics and More.   \n",
    "\n",
    "In a HMM, there are two types of states: \n",
    "-  Hidden states and\n",
    "-  observations states\n",
    "\n",
    "  - There is Underlying state \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e56d2e9",
   "metadata": {},
   "source": [
    "## The OpenAI Gym interface  \n",
    "\n",
    "\n",
    "| Function     | Description           | Important Parameters | Returns   |\n",
    "|--------------|------------------------|----------------------|-----------|\n",
    "| `gym.make()` | Create an environment  | `id`                 | `gym.Env` | \n",
    "|--------------| -----------------------|----------------------|-----------| \n",
    "| gym,Env.reset()| intialize/ Reset the environment |  \n",
    "|--------------|------------------------|----------------------|-----------|\n",
    "| `gym.make()` | Create an environment  | `id`                 | `gym.Env` | \n",
    "|--------------| -----------------------|----------------------|-----------| \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0db6db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.18.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gym) (1.26.2)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gym) (3.0.0)\n",
      "Requirement already satisfied: gym_notices>=0.0.4 in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gym) (0.0.8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ip (C:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (C:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (C:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install gym   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "366d3e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0008be07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "clifEnv =  gym.make(\"CliffWalking-v0\") \n",
    "# gym.make()  function is creates an environment  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d51953",
   "metadata": {},
   "source": [
    "## **Cliff Walking** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96570350",
   "metadata": {},
   "outputs": [],
   "source": [
    "done = False \n",
    "state =  clifEnv.reset() # starting a envoron  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0cb1d72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36, {'prob': 1})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8490704a",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "CliffWalkingEnv.render() got an unexpected keyword argument 'mode'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m st \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done: \n\u001b[1;32m----> 4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mclifEnv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mansi\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m      5\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(low \u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m , high \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m , size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m#print(state, \"--->\" , action)\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m#print(state, \"-->>\" ,[\"up\" , \"right\", \"down\" , \"left\"][action]) \u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gym\\wrappers\\order_enforcing.py:51\u001b[0m, in \u001b[0;36mOrderEnforcing.render\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_disable_render_order_enforcing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\n\u001b[0;32m     48\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call `env.render()` before calling `env.reset()`, if this is a intended action, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     49\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset `disable_render_order_enforcing=True` on the OrderEnforcer wrapper.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     50\u001b[0m     )\n\u001b[1;32m---> 51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gym\\wrappers\\env_checker.py:53\u001b[0m, in \u001b[0;36mPassiveEnvChecker.render\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchecked_render \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchecked_render \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43menv_render_passive_checker\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mrender(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:316\u001b[0m, in \u001b[0;36menv_render_passive_checker\u001b[1;34m(env, *args, **kwargs)\u001b[0m\n\u001b[0;32m    310\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    311\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m env\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m env\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;129;01min\u001b[39;00m render_modes, (\n\u001b[0;32m    312\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe environment was initialized successfully however with an unsupported render mode. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    313\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRender mode: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00menv\u001b[38;5;241m.\u001b[39mrender_mode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, modes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrender_modes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    314\u001b[0m         )\n\u001b[1;32m--> 316\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;66;03m# TODO: Check that the result is correct\u001b[39;00m\n\u001b[0;32m    320\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[1;31mTypeError\u001b[0m: CliffWalkingEnv.render() got an unexpected keyword argument 'mode'"
     ]
    }
   ],
   "source": [
    "st = 0 \n",
    "while not done: \n",
    "\n",
    "    print(clifEnv.render(mode = \"ansi\"))\n",
    "    action = int(np.random.randint(low =0 , high = 4 , size = 1))\n",
    "    #print(state, \"--->\" , action)\n",
    "    #print(state, \"-->>\" ,[\"up\" , \"right\", \"down\" , \"left\"][action]) \n",
    "    st = st +1 \n",
    "    state, reward , done , _ = clifEnv.step(action)\n",
    "\n",
    "# Env.step takes action and give 4 output observation , reward , Done , Info \n",
    "# Env.step give renders a representation of the current state model = {\"single_rgb_array\", \"rgb_array\" , \"human\"} \n",
    "clifEnv.close() # closes the environment \n",
    "# actions --> \n",
    "print(st)\n",
    "\"\"\" \n",
    "Actions -> \n",
    "0 moves up \n",
    "1 move right \n",
    "2 moves down \n",
    "3 move left \n",
    "\n",
    "\"\"\"  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787aafc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_frame() : ... \n",
    "\n",
    "def put_agent(img , state) : ... \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973c9bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "done = False \n",
    "state =  cliffEnv.reset() \n",
    "while not done : \n",
    "    print(clifEnv.render(mode = \"ansi\")) \n",
    "    action = int(np.random.randint(low =0 , high = 4 , size = 1)) \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a7078c",
   "metadata": {},
   "source": [
    "## SARSA  AGENT: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f4072964",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym \n",
    "import numpy as np \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "20267b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "cliffEnv  = gym.make(\"CliffWalking-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e6c24fb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table = np.zeros(shape =(48 , 4))  \n",
    "q_table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "77368842",
   "metadata": {},
   "outputs": [],
   "source": [
    "done =  False  \n",
    "state = cliffEnv.reset()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e125e0ed",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "policy() got an unexpected keyword argument 'EPSILON'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m EPSILON \u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m\n\u001b[0;32m     11\u001b[0m state \u001b[38;5;241m=\u001b[39m cliffEnv\u001b[38;5;241m.\u001b[39mreset() \n\u001b[1;32m---> 12\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[43mpolicy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEPSILON\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m \n",
      "\u001b[1;31mTypeError\u001b[0m: policy() got an unexpected keyword argument 'EPSILON'"
     ]
    }
   ],
   "source": [
    "def policy(state , explore = 0.0) :  \n",
    "    action =  int(np,argmax(q_tabel[state])) \n",
    "    if np.random.random() <=  explore:\n",
    "        action = int(np.random.randint(low =0 , high = 1 , size = 1  ))\n",
    "\n",
    "    return action \n",
    "\n",
    "done =  False  \n",
    "state = cliffEnv.reset()   \n",
    "EPSILON =0.1\n",
    "state = cliffEnv.reset() \n",
    "action = policy(state , EPSILON =0.1) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4fc3a6",
   "metadata": {},
   "source": [
    "## Problem Statement: You going out, Do you need umbrella ? \n",
    "\n",
    "- **State:** Rainy , Cloudy and Sunny\n",
    "-  **Actions**: Umbrell & no umbrella\n",
    "-  **Reward** : Dict mapping with each state action pair\n",
    "-  Transition probability \n",
    "-  Discount factor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "23f27c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "22570c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "states =  ['Rainy' , 'Cloudy' , 'Sunny'] \n",
    "actions = ['umbrella' , 'no umbrella'] \n",
    "rewards = { 'Rainy' :  {'umbrella:' : -1 , \"no umbrella\" : -5 } ,\n",
    "           'Cloudy' :  {'umbrella:' : -1 , \"no umbrella\" : -1 } ,\n",
    "           'Rainy' :  {'umbrella:' : -5 , \"no umbrella\" : -1 } ,       \n",
    "          \n",
    "          } \n",
    "\n",
    "transation =  {\n",
    "    'Rainy' : {\"umbrella\" : { 'Rainy' : 0.7  , 'Cloudy' : 0.3 , 'Sunny' : 0}, \n",
    "               \"no umbrella\" : { 'Rainy' : 0.3  , 'Cloudy' : 0.5 , 'Sunny' : 0.3}} ,\n",
    "    'Cloudy' : {\"umbrella\" : { 'Rainy' : 0.4  , 'Cloudy' : 0.6, 'Sunny' : 0}, \n",
    "               \"no umbrella\" : { 'Rainy' : 0 , 'Cloudy' : 0.7 , 'Sunny' : 0}} ,\n",
    "    'Sunny' : {\"umbrella\" : { 'Rainy' : 0  , 'Cloudy' : 0., 'Sunny' : 1}, \n",
    "               \"no umbrella\" : { 'Rainy' : 0.7  , 'Cloudy' : 0.4 , 'Sunny' : 0.6}} \n",
    "              \n",
    "               \n",
    "               \n",
    "}\n",
    "\n",
    "discountfactor = 0.9  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "105cd0e0",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'umbrella'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 21\u001b[0m\n\u001b[0;32m     17\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m new_V \n\u001b[0;32m     18\u001b[0m             V \u001b[38;5;241m=\u001b[39m  new_V \n\u001b[1;32m---> 21\u001b[0m V \u001b[38;5;241m=\u001b[39m \u001b[43mvalue_iteration\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \n",
      "Cell \u001b[1;32mIn[20], line 10\u001b[0m, in \u001b[0;36mvalue_iteration\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m values \u001b[38;5;241m=\u001b[39m []  \n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m actions : \n\u001b[1;32m---> 10\u001b[0m     value  \u001b[38;5;241m=\u001b[39m \u001b[43mrewards\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43ma\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m s2 \u001b[38;5;129;01min\u001b[39;00m states : \n\u001b[0;32m     12\u001b[0m         value \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m discountfactor \u001b[38;5;241m*\u001b[39m transation[s][a][s2] \u001b[38;5;241m*\u001b[39m V[s2]\n",
      "\u001b[1;31mKeyError\u001b[0m: 'umbrella'"
     ]
    }
   ],
   "source": [
    "def value_iteration() :  \n",
    "    #init the value fn for each state to 0\n",
    "    V = {s : 0  for s in states} \n",
    "    while True  : \n",
    "        #compute the updated value fn for each state \n",
    "        new_V = {} \n",
    "        for s in states : \n",
    "            values = []  \n",
    "            for a in actions : \n",
    "                value  = rewards[s][a]\n",
    "                for s2 in states : \n",
    "                    value += discountfactor * transation[s][a][s2] * V[s2]\n",
    "                values.append(value) \n",
    "            new_V = max(values) \n",
    "            #check for convergence \n",
    "            if all(abs(V[s] - new_V[s]) < 0.0001 for s in states) :  \n",
    "                return new_V \n",
    "            V =  new_V \n",
    "            \n",
    "\n",
    "V = value_iteration() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f798d362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Rainy': -4.674920787172077, 'Cloudy': -2.7027010758234944, 'Sunny': -10.691570578450396}\n"
     ]
    }
   ],
   "source": [
    "states = ['Rainy', 'Cloudy', 'Sunny']\n",
    "actions = ['umbrella', 'no umbrella']\n",
    "\n",
    "rewards = {\n",
    "    'Rainy': {'umbrella': -1, 'no umbrella': -5},\n",
    "    'Cloudy': {'umbrella': -1, 'no umbrella': -1},\n",
    "    'Sunny': {'umbrella': -5, 'no umbrella': -1}\n",
    "}\n",
    "\n",
    "transition = {\n",
    "    'Rainy': {\n",
    "        'umbrella': {'Rainy': 0.7, 'Cloudy': 0.3, 'Sunny': 0},\n",
    "        'no umbrella': {'Rainy': 0.3, 'Cloudy': 0.5, 'Sunny': 0.3}\n",
    "    },\n",
    "    'Cloudy': {\n",
    "        'umbrella': {'Rainy': 0.4, 'Cloudy': 0.6, 'Sunny': 0},\n",
    "        'no umbrella': {'Rainy': 0, 'Cloudy': 0.7, 'Sunny': 0}\n",
    "    },\n",
    "    'Sunny': {\n",
    "        'umbrella': {'Rainy': 0, 'Cloudy': 0, 'Sunny': 1},\n",
    "        'no umbrella': {'Rainy': 0.7, 'Cloudy': 0.4, 'Sunny': 0.6}\n",
    "    }\n",
    "}\n",
    "\n",
    "discountfactor = 0.9\n",
    "\n",
    "def value_iteration():\n",
    "    V = {s: 0 for s in states}\n",
    "    while True:\n",
    "        new_V = {}\n",
    "        for s in states:\n",
    "            values = []\n",
    "            for a in actions:\n",
    "                value = rewards[s][a]\n",
    "                for s2 in states:\n",
    "                    value += discountfactor * transition[s][a][s2] * V[s2]\n",
    "                values.append(value)\n",
    "            new_V[s] = max(values)\n",
    "        if all(abs(V[s] - new_V[s]) < 0.0001 for s in states):\n",
    "            return new_V\n",
    "        V = new_V\n",
    "\n",
    "V = value_iteration()\n",
    "print(V)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f71e39f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal policy:\n",
      "{'Rainy': 'umbrella', 'Cloudy': 'no umbrella', 'Sunny': 'no umbrella'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1a28e14c380>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "policy = {}\n",
    "for s in states:\n",
    "    values = []\n",
    "    for a in actions:\n",
    "        value = rewards[s][a]\n",
    "        for s2 in states:\n",
    "            value += discountfactor * transition[s][a][s2] * V[s2]\n",
    "        values.append(value)\n",
    "    policy[s] = actions[np.argmax(values)]\n",
    "\n",
    "print(\"Optimal policy:\")\n",
    "print(policy)\n",
    "\n",
    "# Plot\n",
    "policy_values = np.zeros((len(states), len(actions)))\n",
    "for i, s in enumerate(states):\n",
    "    for j, a in enumerate(actions):\n",
    "        policy_values[i, j] = rewards[s][a] + discountfactor * sum(transition[s][a][s2] * V[s2] for s2 in states)\n",
    "plt.imshow(policy_values, cmap='Greys')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b53432",
   "metadata": {},
   "source": [
    "\n",
    "### A hidden markov model (HMM) is statistical model used to represent a stochastic process, where the underlying system is assumed to be a markov process with unknown parameter. HMMs are widely used in fields such as signal processibng , speech recognition , NLP , bioinformatics and More."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "11f6bad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting hmmlearn\n",
      "  Downloading hmmlearn-0.3.3-cp312-cp312-win_amd64.whl.metadata (3.1 kB)\n",
      "Requirement already satisfied: numpy>=1.10 in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from hmmlearn) (1.26.2)\n",
      "Requirement already satisfied: scikit-learn!=0.22.0,>=0.16 in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from hmmlearn) (1.4.2)\n",
      "Requirement already satisfied: scipy>=0.19 in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from hmmlearn) (1.13.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn!=0.22.0,>=0.16->hmmlearn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn!=0.22.0,>=0.16->hmmlearn) (3.4.0)\n",
      "Downloading hmmlearn-0.3.3-cp312-cp312-win_amd64.whl (127 kB)\n",
      "Installing collected packages: hmmlearn\n",
      "Successfully installed hmmlearn-0.3.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ip (C:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (C:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (C:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install hmmlearn  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4b4db594",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from hmmlearn import hmm \n",
    "states = ['Rainy', 'Cloudy', 'Sunny']\n",
    "n_states = len(states)\n",
    "observations = ['Umbrella', 'No Umbrella']\n",
    "n_observations = len(observations) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "999a515e",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_probability = np.array([0.33, 0.33, 0.34])\n",
    "transition_probability = np.array([[0.7, 0.3, 0.0],\n",
    "                                 [0.4, 0.6, 0.0],\n",
    "                                 [0.0, 0.4, 0.6]])\n",
    "emission_probability = np.array([[0.5, 0.5],\n",
    "                                [0.9, 0.1],\n",
    "                                [0.1, 0.9]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "16d4db53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MultinomialHMM has undergone major changes. The previous version was implementing a CategoricalHMM (a special case of MultinomialHMM). This new implementation follows the standard definition for a Multinomial distribution (e.g. as in https://en.wikipedia.org/wiki/Multinomial_distribution). See these issues for details:\n",
      "https://github.com/hmmlearn/hmmlearn/issues/335\n",
      "https://github.com/hmmlearn/hmmlearn/issues/340\n"
     ]
    }
   ],
   "source": [
    "#create hmm object\n",
    "model = hmm.MultinomialHMM(n_components=n_states, n_iter=100)\n",
    "model.n_trials = 10\n",
    "model.startprob_ = start_probability\n",
    "model.transmat_ = transition_probability\n",
    "model.emissionprob_ = emission_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "88b11b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0,0,], [1,1], [1,0], [0,1]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9e2d3929",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states = model.predict(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c172d57f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rainy\n",
      "Rainy\n",
      "Rainy\n",
      "Rainy\n"
     ]
    }
   ],
   "source": [
    "for s in hidden_states:\n",
    "    print(states[s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93694eec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
